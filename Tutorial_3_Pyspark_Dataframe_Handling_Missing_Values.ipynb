{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 3- Pyspark Dataframe- Handling Missing Values.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6RyYDF1BMs75fHxxGsBo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shraddha-Ramteke/PySpark-Tutorial/blob/main/Tutorial_3_Pyspark_Dataframe_Handling_Missing_Values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Handling Missing Values\n",
        "- Dropping Columns\n",
        "- Dropping Rows\n",
        "- Various Parameter In Dropping functionalities\n",
        "- Handling Missing values by Mean, MEdian And Mode\n",
        "\n"
      ],
      "metadata": {
        "id": "ORrmn_mKhdCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Practise').getOrCreate()"
      ],
      "metadata": {
        "id": "mwhQCEd7hxuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "OA7QrnoLhymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "-JqUtiLBh1bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "soRcUIyUh30h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##drop the columns\n",
        "df_pyspark.drop('Name').show()"
      ],
      "metadata": {
        "id": "kW0sfH99h6W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "aORUFENlh8w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.drop().show()"
      ],
      "metadata": {
        "id": "JUny8J2Uh_7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### any==how\n",
        "df_pyspark.na.drop(how=\"any\").show()"
      ],
      "metadata": {
        "id": "MwbDIkEFiCKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##threshold\n",
        "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
      ],
      "metadata": {
        "id": "_pX5BmUFiHcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Subset\n",
        "df_pyspark.na.drop(how=\"any\",subset=['Age']).show()"
      ],
      "metadata": {
        "id": "M30dpbKbiKG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Filling the Missing Value\n",
        "df_pyspark.na.fill('Missing Values',['Experience','age']).show()"
      ],
      "metadata": {
        "id": "qDmGAsSNiMon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "RI9P2ggEiP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "L_3WkjfRiUSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'], \n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")"
      ],
      "metadata": {
        "id": "8iSW-6ChiiaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add imputation cols to df\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()\n",
        " "
      ],
      "metadata": {
        "id": "vkkPbaleilfQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}